{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bce26a",
   "metadata": {},
   "source": [
    "# Reddit Posts Analysis with Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ccebc",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df4731",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208dc1f",
   "metadata": {},
   "source": [
    "Pet ownership is often viewed positively in the mainstream media, especially during the COVID-19 pandemic. However, these perspectives can sometimes be overstated and biased, creating unrealistic expectations for pet owners. In 2022, a US-based survey revealed that pets were a major source of stress for a majority of owners during the pandemic. ([source](https://www.avma.org/news/study-explores-pandemic-specific-challenges-pet-ownership)) Hence, a follow-up analysis is required to understand pet owners' pain points to ensure the right measure can be taken to reduce stress and improve their experience.\n",
    "\n",
    "In this project, we'll be assuming the role of pro-bono data analysts for an animal shelter to address:\n",
    "1. What are the top concerns for pet owners, specifically dog and cat owners?\n",
    "2. Which classification model, along with the natural language processing, would be able to predict cat- or dog-specific problems? \n",
    "\n",
    "To answer these questions, we'll mine for user-generated content in Reddit and analyze them through natural language processing techniques. Thereafter, we'll provide the animal shelter with recommendations on how they can better support potential pet owners and ensure a positive experience for both owners and pets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8302cb",
   "metadata": {},
   "source": [
    "*Content:*\n",
    "\n",
    "1. [Executive Summary](#Executive-Summary)\n",
    "2. [Data Collection & Methodology](#Data-Collection-&-Methodology)\n",
    "3. [Web Scraping](#Web-Scraping)\n",
    "\n",
    "*Please refer to the next notebook for remaining content, click [here](Part_2_Reddit_Analysis_NLP.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42983e85",
   "metadata": {},
   "source": [
    "## Data Collection & Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2472dca",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "We collected 6,000 posts from two popular subreddits:\n",
    "\n",
    "1. [r/CatAdvice](https://www.reddit.com/r/CatAdvice/): This subreddit has over 118,000* cat enthusiasts globally, who actively discuss cat-care topics and exchange best tips for their cats. A total of 3,000 posts were scraped from this subreddit, along with 72 potential feature variables.\n",
    "2. [r/DogAdvice](https://www.reddit.com/r/DogAdvice/): With over 65,900* members globally, this subreddit is dedicated to discussions on dog training, health, nutrition and other dog-care topics. A total of 3,000 posts were scraped from this subreddit, as well as 80 potential feature variables.\n",
    "\n",
    "*figures are latest as of November 2022 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c8c80",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "After cleaning the data we narrowed now to 2,746 data from r/CatAdvice and 1,353 in r/DogAdvice.\n",
    "\n",
    "Based on the 72-80 variables pulled, we also narrowed down to a few key features that were pertinent in our investigation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72138674",
   "metadata": {},
   "source": [
    "| Selected Feature \t| Type    \t| Dataset                     \t| Description                                      \t|\n",
    "|------------------\t|---------\t|-----------------------------\t|--------------------------------------------------\t|\n",
    "| subreddit        \t| object  \t| r/CatAdvice and r/DogAdvice \t| Refers to the Reddit channel                     \t|\n",
    "| title            \t| object  \t| r/CatAdvice and r/DogAdvice \t| Reddit title of the post                         \t|\n",
    "| selftext         \t| object  \t| r/CatAdvice and r/DogAdvice \t| Reddit body post                                 \t|\n",
    "| author           \t| object  \t| r/CatAdvice and r/DogAdvice \t| Reddit member                                    \t|\n",
    "| created_utc      \t| integer \t| r/CatAdvice and r/DogAdvice \t| Time of when the post was created, in UTC format \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150a97d",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "The subreddit posts were processed and analysed through the following steps:\n",
    "\n",
    "1. **Web Scraping**: Using Pushshift API, we were able to extract 100 posts per day within 30 days from the subreddits.\n",
    "2. **Data Cleaning**: We assessed the data by checking the relevance of the variables, removing outliers, dealing with missing values that were key to the investigation.\n",
    "3. **Exploratory Data Analysis**: We visualized the cleaned data through a series of graphs and plots to better understand the dataset and identify potential keywords for the modeling process.\n",
    "4. **Data Modeling & Evaluation**: Following the selection of variables, we processed the text data even further and modeled them through these models - Logistic Regression, Naive Bayes and XXX. The best model will then be used to predict whether keywords that would best identify cat vs dog issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e648416",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc51bbd",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318d7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80f55c",
   "metadata": {},
   "source": [
    "### Set up requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6379a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df159dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 completed\n",
      "Iteration 1 completed\n",
      "Iteration 2 completed\n",
      "Iteration 3 completed\n",
      "Iteration 4 completed\n",
      "Iteration 5 completed\n",
      "Iteration 6 completed\n",
      "Iteration 7 completed\n",
      "Iteration 8 completed\n",
      "Iteration 9 completed\n",
      "Iteration 10 completed\n",
      "Iteration 11 completed\n",
      "Iteration 12 completed\n",
      "Iteration 13 completed\n",
      "Iteration 14 completed\n",
      "Iteration 15 completed\n",
      "Iteration 16 completed\n",
      "Iteration 17 completed\n",
      "Iteration 18 completed\n",
      "Iteration 19 completed\n",
      "Iteration 20 completed\n",
      "Iteration 21 completed\n",
      "Iteration 22 completed\n",
      "Iteration 23 completed\n",
      "Iteration 24 completed\n",
      "Iteration 25 completed\n",
      "Iteration 26 completed\n",
      "Iteration 27 completed\n",
      "Iteration 28 completed\n",
      "Iteration 29 completed\n"
     ]
    }
   ],
   "source": [
    "# Pull 100 posts per day for 30 days from r/CatAdvice and r/DogAdvice and save into dataframes\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < 30: #pull from API 30 times\n",
    "    before = i \n",
    "    cat_dict = requests.get(url, params = {'subreddit' : 'CatAdvice', 'size' : 100, 'before' : f\"{before}d\"}).json()\n",
    "    dog_dict = requests.get(url, params = {'subreddit' : 'DogAdvice', 'size' : 100, 'before' : f\"{before}d\"}).json()\n",
    "    print(f\"Iteration {i} completed\")\n",
    "    \n",
    "    if i == 0:\n",
    "        cat_df = pd.DataFrame(cat_dict['data'])\n",
    "        dog_df = pd.DataFrame(dog_dict['data'])\n",
    "    else:\n",
    "        cat_df = pd.concat([cat_df, pd.DataFrame(cat_dict['data'])], ignore_index=True, axis = 0)\n",
    "        dog_df = pd.concat([dog_df, pd.DataFrame(dog_dict['data'])], ignore_index=True, axis = 0)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    time.sleep(random.uniform(5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a846ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 72)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for the size of data pulled from CatAdvice\n",
    "\n",
    "cat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37f0522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 80)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for the size of data pulled from DogAdvice\n",
    "\n",
    "dog_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6140dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CatAdvice raw data into csv\n",
    "\n",
    "cat_df.to_csv('../datasets/cat_advice_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7149da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DogAdvice raw data into csv\n",
    "\n",
    "dog_df.to_csv('../datasets/dog_advice_raw.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
